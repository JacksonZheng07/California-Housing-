# -*- coding: utf-8 -*-
"""California_Housing Linear Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lGVJd1-ejJy-Y_IjjfdRMuBY_KLp-Gy9
"""

from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Load data
housing = fetch_california_housing()
X = housing.data
y = housing.target

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Utlizing Gradient Descent
mean = np.mean(X_train, axis = 0)
std = np.std(X_train, axis = 0)

#scaling
X_train_scaled = (X_train - mean) / std
X_test_scaled = (X_test - mean) / std

#adding bias
X_train_bias = np.insert(X_train_scaled, 0, 1, axis = 1)
X_test_bias = np.insert(X_test_scaled, 0, 1, axis = 1)

#Setting up Beta
Beta = np.zeros(X_train_bias.shape[1])

#Hyper Perm setting up
learning_rate = 0.01
epochs = 100001

#Gradient Descent Function
def Gradient_descent(X, y, beta, alpha, l, epochs):
    m = len(y)
    losses = []

    for epoch in range(epochs):
        y_pred = X @ beta
        error = y_pred - y

        gradient = X.T @ error / m + beta * alpha
        beta -= l * gradient

        loss = np.mean(error ** 2)
        losses.append(loss)

        if epoch % 50000 == 0:
            ss_res = np.sum((y - y_pred) ** 2)
            ss_tot = np.sum((y - np.mean(y)) ** 2)
            r2 = 1 - ss_res / ss_tot
            print(f"Epoch {epoch}, Loss: {loss}, R2: {r2}")

    return beta, losses


beta, losses = Gradient_descent(X_train_bias, y_train, Beta, 0.001, learning_rate, epochs)

# Plot training loss
plt.plot(losses)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss over Epochs")
plt.show()

# Predict on test data
y_pred_test = X_test_bias @ Beta

# Evaluation
mse = np.mean((y_test - y_pred_test) ** 2)
mae = np.mean(np.abs(y_test - y_pred_test))
ss_res = np.sum((y_test - y_pred_test) ** 2)
ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)
r2 = 1 - ss_res / ss_tot

print(f"Test MSE: {mse}")
print(f"Test MAE: {mae}")
print(f"Test R2: {r2}")

#Creating Linear Regression
X_train = np.insert(X_train, 0, 1, axis=1)
Beta = np.linalg.inv(X_train.T @ X_train) @ (np.dot(X_train.T, y_train))

#Testing Accuracy of the Regression Line
print(X_test.shape)
print(Beta.shape)

y_pred = X_train @ Beta

mse = np.mean((y_train - y_pred)**2) * (1/len(y_train))
print("MSE:", mse)

R2 = 1 - np.mean((y_train - y_pred)**2) / np.mean((y_train - np.mean(y_train)) **2)
print("R2:", R2)

#Testing Accuracy on testing data
X_test_with_bias = np.insert(X_test, 0, 1, axis=1)

y_pred_test = X_test_with_bias @ Beta
mse = np.mean((y_test - y_pred_test)**2) * (1/len(y_train))
print("MSE:", mse)

R2 = 1 - np.mean((y_test - y_pred_test)**2) / np.mean((y_test - np.mean(y_test)) **2)
print("R2:", R2)